{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHuKj1qd21Ub",
        "outputId": "092af29c-32f6-4fa4-9536-4e8154f798ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Looks like the sun finally located Trondheim ;-) hope summer's on it's way \", \"A long weekend begins. The sun is shining and I'm happy ! Exams soon \", 'to the beach we go! hope it stays nice... ', '@JBFutureboy I missed it  busted need to do a reunion tour. That would make my year. No joke.', \"Why I can't change my background image?? \"]\n",
            "['positif', 'négatif', 'positif', 'négatif', 'négatif']\n"
          ]
        }
      ],
      "source": [
        "# Extraction depuis CSV\n",
        "import csv\n",
        "tweets_data = []  # On initialise une liste vide pour stocker les tweets et leurs labels\n",
        "with open(\"balanced_tweets_200k.csv\", \"r\", encoding=\"latin1\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        if len(row) >= 6:\n",
        "            raw_label = row[0]\n",
        "            tweet = row[5]\n",
        "\n",
        "            # Conversion du label brut en label textuel (positif/négatif)\n",
        "            if raw_label == \"0\":\n",
        "                label = \"négatif\"\n",
        "            elif raw_label == \"4\":\n",
        "                label = \"positif\"\n",
        "            else:\n",
        "                continue\n",
        "            tweets_data.append((tweet, label))\n",
        "\n",
        "# Extraction des colonnes\n",
        "raw_tweets = [t[0] for t in tweets_data]\n",
        "labels = [t[1] for t in tweets_data]\n",
        "print(raw_tweets[:5])\n",
        "print(labels[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re #pour utiliser expressions régulières\n",
        "from pathlib import Path #pour manipuler les fichiers\n",
        "\n",
        "import nltk #notre bibliothèque de traitement de texte documenté\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TreebankWordTokenizer #tokenizer : transforme les phrases en liste de mots\n",
        "\n",
        "# Télécharger les ressources NLTK\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "# Initialisation du tokenizer (plus robuste que word_tokenize)\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "# Préparation des regex\n",
        "# on supprime les artefacts typiques des tweets encodés de manière étrange\n",
        "regex_artifacts_sources = re.compile(\n",
        "    r'\\|'\n",
        "    r'\\[Saut de retour à la ligne\\]|'\n",
        "    r'(?:<ed><U\\+[0-9A-F]{4}><U\\+[0-9A-F]{4}>)+|'\n",
        "    r'<U\\+[0-9A-F]{4,6}>|'\n",
        "    r'<ed>'\n",
        ")\n",
        "\n",
        "# on supprime les noms d'utilisateurs mentionnés\n",
        "regex_usernames = re.compile(r'@\\w+')\n",
        "\n",
        "# on supprime les URL (http ou https)\n",
        "regex_urls = re.compile(r'https?://\\S+')\n",
        "\n",
        "# on supprime les hashtags uniquement\n",
        "regex_hashtags = re.compile(r'#\\w+')\n",
        "\n",
        "# on supprime les contractions comme l', j', c', etc.\n",
        "regex_apostrophes = re.compile(r\"\\b\\w+'\")\n",
        "\n",
        "# on supprime les guillemets doubles\n",
        "regex_quotes = re.compile(r'\"')\n",
        "\n",
        "# on supprime la ponctuation générale\n",
        "regex_punctuation = re.compile(r'[.,;:!?()\\[\\]{}\\\\/|`~^<>«»=]')\n",
        "\n",
        "# Traitement des fichiers\n",
        "\n",
        "# Crée le dossier de sortie s’il n’existe pas\n",
        "#output_dir = Path(\"CorpusRandomCleaned\")\n",
        "#output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Définition du chemin de sortie pour les tweets nettoyés\n",
        "#dest_file = output_dir / f\"cleaned_tweets{i}.txt\"\n",
        "\n",
        "cleaned_tweets = []\n",
        "\n",
        "# Choix de la langue pour les stopwords\n",
        "langue = \"english\"\n",
        "stop_words = set(stopwords.words(langue))\n",
        "\n",
        "for tweet_text in raw_tweets:\n",
        "\n",
        "    # Nettoyage du texte\n",
        "    # Supprime les artefacts liés à l'encodage\n",
        "    t = regex_artifacts_sources.sub('', tweet_text)\n",
        "    # Corrige les doubles guillemets (\"\" → \")\n",
        "    t = t.replace('\"\"', '\"')\n",
        "    # Supprime les noms d’utilisateurs\n",
        "    t = regex_usernames.sub('', t)\n",
        "    # Supprime les liens\n",
        "    t = regex_urls.sub('', t)\n",
        "    # Supprime les hashtags\n",
        "    t = regex_hashtags.sub('', t)\n",
        "    # Supprime les contractions du type l', j', etc.\n",
        "    t = regex_apostrophes.sub('', t)\n",
        "    # Supprime tous les guillemets\n",
        "    t = regex_quotes.sub('', t)\n",
        "    # Supprime les apostrophes restantes\n",
        "    t = t.replace(\"'\", \"\")\n",
        "    # Supprime la ponctuation\n",
        "    t = regex_punctuation.sub(' ', t)\n",
        "    # Met en minuscule\n",
        "    t = t.lower()\n",
        "    # Supprime les espaces multiples\n",
        "    t = re.sub(r'\\s{2,}', ' ', t).strip()\n",
        "\n",
        "    # Tokenisation\n",
        "    tokens = tokenizer.tokenize(t)\n",
        "\n",
        "    # Conservation de RT en majuscule même après passage en minuscule\n",
        "    filtered_tokens = [\n",
        "        word.upper() if word.lower() == 'rt' else word\n",
        "        for word in tokens\n",
        "        if word.lower() not in stop_words or word.lower() == 'rt'\n",
        "    ]\n",
        "\n",
        "    # On garde le tweet sous forme de liste de mots\n",
        "    cleaned_tweets.append(f\"{filtered_tokens}\")\n",
        "\n",
        "# Sauvegarde du fichier nettoyé\n",
        "#with open(dest_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#    f.write(\"\\n\".join(cleaned_tweets))"
      ],
      "metadata": {
        "id": "IrD5ptiY4Tqi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-E-UapJ3Dmwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6702d08c-b63b-44ce-e2a8-e7bbd717ed77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cuml.accel extension is already loaded. To reload it, use:\n",
            "  %reload_ext cuml.accel\n"
          ]
        }
      ],
      "source": [
        "#Importation des fonctions et librairies\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Externalisation des entraînements de modèles et inférences sur GPU et non CPU\n",
        "\n",
        "%load_ext cuml.accel\n",
        "\n",
        "# Calcule le score TF-IDF automatiquement\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    max_features=15000,      # Limite le vocabulaire\n",
        "    min_df=5,               # Ignore les mots présents dans <5 documents\n",
        "    max_df=0.7,             # Ignore les mots présents dans >70% des documents\n",
        "    sublinear_tf=True,\n",
        "    analyzer='word',\n",
        "    stop_words='english'    # Utilise la liste de stop words intégrée\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "01tl03lFwc_S"
      },
      "outputs": [],
      "source": [
        "# On appelle la fonction TF-IDF sur nos données\n",
        "\n",
        "data = tfidf.fit_transform(cleaned_tweets)\n",
        "#data = np.array(cleaned_tweets)\n",
        "X = data\n",
        "Y = [1 if label == \"positif\" else -1 for label in labels]\n",
        "n = data.shape[0] # nombre de données (utilisé pour n'entraîner que sur une partie pour des petits tests)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[:n], Y[:n], test_size=0.2, random_state=42) # séparation des datasets d'entraînement + validation (cross-validation) et de test, avec du déterminisme (seed fixée)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze-v7gxqr6-A"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Te7uP3xSjX"
      },
      "source": [
        "Régression logistique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8oNxULfQxJED",
        "outputId": "1d67f843-7a9c-49ee-e57c-e457110a3ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-05 08:15:19.304] [CUML] [info] Unused keyword parameter: random_state during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.325] [CUML] [info] Unused keyword parameter: dual during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.325] [CUML] [info] Unused keyword parameter: intercept_scaling during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.325] [CUML] [info] Unused keyword parameter: multi_class during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.325] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.325] [CUML] [info] Unused keyword parameter: random_state during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.325] [CUML] [info] Unused keyword parameter: warm_start during cuML estimator initialization\n",
            "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
            "[2025-06-05 08:15:19.343] [CUML] [info] Unused keyword parameter: dual during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.343] [CUML] [info] Unused keyword parameter: intercept_scaling during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.343] [CUML] [info] Unused keyword parameter: multi_class during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.343] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.343] [CUML] [info] Unused keyword parameter: random_state during cuML estimator initialization\n",
            "[2025-06-05 08:15:19.343] [CUML] [info] Unused keyword parameter: warm_start during cuML estimator initialization\n",
            "{'C': 1, 'penalty': 'l2'}\n",
            "Test set accuracy: 76.41%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,               # Régularisation\n",
        "    #l1_ratio=0.4,  # Ratio de régularisation L1\n",
        "    class_weight='balanced',  # Équilibrage des classes\n",
        "    solver='liblinear',  # Algorithme de résolution\n",
        "    max_iter=1000,       # Nombre maximal d'itérations\n",
        "    random_state=42      # Pour la reproductibilité\n",
        ")\n",
        "\n",
        "# Convertit y_train et y_test en NumPy arrays avec un dtype spécifique pour éviter les erreurs et map -1 à 0\n",
        "y_train_np = np.array([(1 if y == 1 else 0) for y in y_train], dtype=np.int32)\n",
        "y_test_np = np.array([(1 if y == 1 else 0) for y in y_test], dtype=np.int32)\n",
        "\n",
        "# On définit la grille de tests d'hyperparamètres pour la cross-validation\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "grid = GridSearchCV(model, param_grid, cv=10, scoring='accuracy', verbose=1) #cross validation\n",
        "grid.fit(X_train, y_train_np) #entraînement\n",
        "print(grid.best_params_)\n",
        "best_model = grid.best_estimator_ #on choisit le meilleur\n",
        "best_model.fit(X_train, y_train_np) #on le ré-entraîne sur tout\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = best_model.score(X_test, y_test_np)\n",
        "print(\"Test set accuracy: {:.2f}%\".format((accuracy) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CM-6DKDxn2j"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEUn2pjzxZWo",
        "outputId": "bb9a65d5-8adf-4989-8b91-e2dd924d83d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cupy/_creation/from_data.py:88: PerformanceWarning: Using synchronous transfer as pinned memory (19200000000 bytes) could not be allocated. This generally occurs because of insufficient host memory. The original error was: cudaErrorMemoryAllocation: out of memory\n",
            "  return _core.array(a, dtype, False, order, blocking=blocking)\n"
          ]
        }
      ],
      "source": [
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,  # Nombre d'arbres\n",
        "    criterion='gini',    # Critère de split (gini ou entropy)\n",
        "    max_depth=None,      # Profondeur maximale de l'arbre\n",
        "    min_samples_split=2, # Nombre minimum d'échantillons requis pour spliter un nœud interne\n",
        "    min_samples_leaf=1,  # Nombre minimum d'échantillons requis à un nœud feuille\n",
        "    random_state=42      # Pour la reproductibilité\n",
        ")\n",
        "\n",
        "# On convertit les \"sparse matrices\" en \"dense arrays\" pour ne pas avoir d'erreurs avec cuML\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "# On convertit y_train et y_test en NumPy arrays avec le bon dtype\n",
        "y_train_np = np.array(y_train, dtype=np.float32)\n",
        "y_test_np = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "rf_model.fit(X_train_dense, y_train_np) #entraînement\n",
        "\n",
        "# NB : on peut refaire de la cross-validation ici aussi, typiquement sur le nombre d'arbres ou le critère\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test_dense)\n",
        "\n",
        "accuracy_rf = rf_model.score(X_test_dense, y_test_np)\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQcdolri32EE"
      },
      "source": [
        "Mixture of Experts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzbk163E34xF"
      },
      "outputs": [],
      "source": [
        "# Definition des modèles experts - on en prend des nouveaux mais on pourrait en récupérer des pré-entraînés\n",
        "expert1 = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1.0,\n",
        "    class_weight='balanced',\n",
        "    solver='liblinear',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "expert2 = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    criterion='gini',\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "expert3 = SVC(probability=True, random_state=42)\n",
        "\n",
        "\n",
        "# On crée le modèles qui vote (Mixture of Experts)\n",
        "# 'voting'='hard' => majority voting, 'voting'='soft' => predicted probabilities\n",
        "moe_model = VotingClassifier(\n",
        "    estimators=[('lr', expert1), ('rf', expert2), ('svm', expert3)],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "# Entraînement (encore et toujours)\n",
        "moe_model.fit(X_train, y_train)\n",
        "\n",
        "# Prédictions\n",
        "y_pred_moe = moe_model.predict(X_test)\n",
        "\n",
        "accuracy_moe = moe_model.score(X_test, y_test)\n",
        "print(f\"Mixture of Experts Accuracy: {accuracy_moe:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Réseau de neurones (élémentaire, pour un plus avancer aller sur RNplusplus (ou les autres RN... déjà))"
      ],
      "metadata": {
        "id": "pQ2UxMFW8tOo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY7mwuM05EWo"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow keras\n",
        "\n",
        "# Encore un petit paquet d'imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# On construit l'architecture du réseau et les fonctions d'activation\n",
        "input_layer = Input(shape=(X_train.shape[1],))\n",
        "dense_layer_1 = Dense(128, activation='relu')(input_layer)\n",
        "dropout_layer_1 = Dropout(0.5)(dense_layer_1)\n",
        "dense_layer_2 = Dense(64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(0.5)(dense_layer_2)\n",
        "output_layer = Dense(1, activation='sigmoid')(dense_layer_2) # Using sigmoid for binary classification\n",
        "\n",
        "nn_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# On génère une instance de ce modèle\n",
        "nn_model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                 loss='binary_crossentropy', # Using binary crossentropy for binary classification\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "# On doit convertir les labels en 0 et 1 pour binary crossentropy\n",
        "y_train_nn = [(1 if y == 1 else 0) for y in y_train]\n",
        "y_test_nn = [(1 if y == 1 else 0) for y in y_test]\n",
        "\n",
        "# Entraînement\n",
        "# On convertit les \"sparse matrix\" en \"dense array\" pour le GPU\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "# Conversion des NumPy arrays en TensorFlow Tensors (changement de librairie oblige)\n",
        "X_train_tensor = tf.convert_to_tensor(X_train_dense, dtype=tf.float32)\n",
        "X_test_tensor = tf.convert_to_tensor(X_test_dense, dtype=tf.float32)\n",
        "y_train_tensor = tf.convert_to_tensor(y_train_nn, dtype=tf.float32)\n",
        "y_test_tensor = tf.convert_to_tensor(y_test_nn, dtype=tf.float32)\n",
        "\n",
        "\n",
        "nn_model.fit(X_train_tensor, y_train_tensor, epochs=50, batch_size=10, verbose=0) #Le vrai entraînement ! On peut bouger le nombre d'epochs ou la batch size\n",
        "\n",
        "# On calcule la précision\n",
        "loss, accuracy_nn = nn_model.evaluate(X_test_tensor, y_test_tensor, verbose=0)\n",
        "print(f\"Neural Network Accuracy: {accuracy_nn:.2%}\")\n",
        "\n",
        "# On regarde ce qu'il se passe sur les labels prédits et réels pour comparer \"à la main\"\n",
        "# Comme la sortie est en probas, on convertit en -1/1\n",
        "y_pred_nn_prob = nn_model.predict(X_test_tensor)\n",
        "y_pred_nn = [(1 if prob > 0.5 else -1) for prob in y_pred_nn_prob]\n",
        "\n",
        "print(\"Neural Network Predictions:\", y_pred_nn)\n",
        "print(\"Actual Test Labels:\", y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (variante RF)"
      ],
      "metadata": {
        "id": "LVrZC4cA8xR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMiScxQ06IxD"
      },
      "outputs": [],
      "source": [
        "#!pip install xgboost\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# XGBoost a besoin de DMatrix en interne\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# On définit les hyperparamètres de XGBoost\n",
        "\n",
        "y_train_xgb = [(1 if y == 1 else 0) for y in y_train] # pour utiliser la binary classification\n",
        "y_test_xgb = [(1 if y == 1 else 0) for y in y_test]\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train_xgb)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test_xgb)\n",
        "\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary:logistic',  # Binary classification avec logistic regression\n",
        "    'eval_metric': 'logloss',        # Métrique\n",
        "    'eta': 0.5,                      # Learning rate\n",
        "    'max_depth': 3,\n",
        "    'subsample': 0.8,                # Pourcentage d'entraînement (80-20%)\n",
        "    'colsample_bytree': 0.8,         # Pourcentage de colonnes utilisées pour un arbre\n",
        "    'seed': 42                       # Reproducibilité de l'aléatoire\n",
        "}\n",
        "\n",
        "# Entraînement de XGBoost\n",
        "num_rounds = 100  # Nombre de boosting rounds\n",
        "watchlist = [(dtrain, 'train'), (dtest, 'eval')] # Permet de surveiller l'évolution de la performance\n",
        "\n",
        "xgb_model = xgb.train(params, dtrain, num_rounds, evals=watchlist, early_stopping_rounds=10, verbose_eval=False) #Entraînement !\n",
        "\n",
        "# Prédictions\n",
        "# XGBoost renvoie des probas avec binary:logistic\n",
        "y_pred_xgb_prob = xgb_model.predict(dtest)\n",
        "\n",
        "# On convertit ces probabilités en labels (0 or 1)\n",
        "y_pred_xgb = [1 if prob > 0.5 else 0 for prob in y_pred_xgb_prob]\n",
        "\n",
        "# On reconvertit au format -1, 1 pour comparer avec y_test\n",
        "y_pred_xgb_original = [1 if pred == 1 else -1 for pred in y_pred_xgb]\n",
        "\n",
        "\n",
        "# On évalue les performances\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_xgb = accuracy_score(y_test_xgb, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Accuracy: {accuracy_xgb:.2%}\")\n",
        "\n",
        "print(\"XGBoost Predictions (0/1):\", y_pred_xgb)\n",
        "print(\"XGBoost Predictions (-1/1):\", y_pred_xgb_original)\n",
        "print(\"Actual Test Labels (-1/1):\", y_test)\n",
        "print(\"Actual Test Labels (0/1):\", y_test_xgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "k Nearest Neighbors"
      ],
      "metadata": {
        "id": "BJjTF-Ue80VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import BallTree\n",
        "print(sorted(BallTree.valid_metrics))\n",
        "# Juste tests pour les métriques ci-dessous (sorte de cross-validation manuelle pour éviter la surcharge de mémoire)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5S5tSeE03me",
        "outputId": "195766d6-d62e-4448-c534-58d23c84772f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['braycurtis', 'canberra', 'chebyshev', 'cityblock', 'dice', 'euclidean', 'hamming', 'haversine', 'infinity', 'jaccard', 'l1', 'l2', 'mahalanobis', 'manhattan', 'minkowski', 'p', 'pyfunc', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "S9CYb1hR6NJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89510d81-d29f-4a2e-968e-5cf480dbae7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-05 21:29:21.218] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:29:45.687] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:29:45.687] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
            "[2025-06-05 21:29:45.707] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:29:45.707] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:29:49.816] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:29:49.816] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:29:53.838] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:29:53.838] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:29:58.625] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:29:58.625] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:02.698] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:02.698] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:07.566] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:07.566] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:12.525] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:12.525] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:16.746] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:16.746] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:20.963] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:20.963] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:26.040] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:26.040] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:30.249] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:30.249] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:34.444] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:34.444] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:39.442] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:39.442] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:43.525] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:43.525] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:47.556] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:47.556] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:52.350] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:52.350] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:30:56.391] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:30:56.391] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:00.506] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:00.506] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:05.175] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:05.175] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:09.189] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:09.189] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:13.329] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:13.329] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:18.123] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:18.123] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:22.303] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:22.303] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:26.635] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:26.636] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:31.336] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:31.336] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:35.544] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:35.544] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:40.239] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:40.240] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:44.966] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:44.966] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:49.193] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:49.193] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:54.187] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:54.187] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:31:58.518] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:31:58.518] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:02.737] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:02.737] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:07.823] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:07.823] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:12.066] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:12.066] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:16.284] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:16.284] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:21.465] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:21.465] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:25.645] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:25.645] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:29.680] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:29.680] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:34.395] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:34.395] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:38.380] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:38.380] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "[2025-06-05 21:32:42.388] [CUML] [info] Unused keyword parameter: leaf_size during cuML estimator initialization\n",
            "[2025-06-05 21:32:42.388] [CUML] [info] Unused keyword parameter: n_jobs during cuML estimator initialization\n",
            "{'n_neighbors': 3}\n",
            "Test set accuracy: 61.92%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "knn_model = KNeighborsClassifier(\n",
        "    n_neighbors=3,  # nombre de voisins à considérer\n",
        "    weights='distance',\n",
        "    algorithm='ball_tree', # Algorithme utilisé pour trouver les voisins\n",
        "    leaf_size=30,     # argument de l'algo\n",
        "    #p=2,              # Ppuissance pour la métrique 'minkovski'\n",
        "    metric='l1', # Distance\n",
        ")\n",
        "\n",
        "# Conversion de y_train et y_test en NumPy arrays\n",
        "y_train_np = np.array(y_train, dtype=np.int32)\n",
        "y_test_np = np.array(y_test, dtype=np.int32)\n",
        "\n",
        "knn_model.fit(X_train, y_train_np) # Entraînement\n",
        "\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "\n",
        "accuracy_knn = knn_model.score(X_test, y_test_np)\n",
        "#print(f\"kNN Accuracy: {accuracy_knn:.2%}\")\n",
        "#print(\"kNN Predictions:\", y_pred_knn)\n",
        "#print(\"Actual Test Labels:\", y_test_np)\n",
        "# Cross validation\n",
        "param_grid = {'n_neighbors': list(range(1, 5))}\n",
        "grid = GridSearchCV(knn_model, param_grid, cv=10, scoring='accuracy', verbose=1)\n",
        "grid.fit(X_train, y_train_np)\n",
        "print(grid.best_params_)\n",
        "best_knn = grid.best_estimator_\n",
        "y_pred = best_knn.predict(X_test)\n",
        "print(\"Test set accuracy: {:.2f}%\".format(accuracy_score(y_test_np, y_pred) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4OCySz96V1z"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine (SVM)\n",
        "svm_model = SVC(\n",
        "    C=1.0,                 # Régularisation (plus C est grand, moins la régularisation est forte)\n",
        "    kernel='linear',       # Type de noyau ('linear', 'poly', 'rbf', 'sigmoid')\n",
        "    gamma='scale',         # Coefficient de noyau pour 'rbf', 'poly', 'sigmoid'\n",
        "    probability=True,      # Permet de calculer les probabilités de classe\n",
        "    random_state=42        # Pour la reproductibilité\n",
        ")\n",
        "\n",
        "svm_model.fit(X_train, y_train) # Entraînement\n",
        "\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "accuracy_svm = svm_model.score(X_test, y_test)\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.2%}\")\n",
        "print(\"SVM Predictions:\", y_pred_svm)\n",
        "print(\"Actual Test Labels:\", y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}